{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20160609_정진하.ipynb","provenance":[{"file_id":"1D5XmduoQ6HDFWCgHqqEgreWAjRvOfcwm","timestamp":1590758618141},{"file_id":"1LYpts1v3YTM8HheW-4YSdJmu8tk2XmZd","timestamp":1590723377075},{"file_id":"1mtf7bW7-9A0t1R0mEjwtAlhk38hQMShy","timestamp":1590487089782}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"4SPQIkPmAFD7","colab_type":"text"},"source":["## import modules & set variables"]},{"cell_type":"code","metadata":{"id":"mUC5Vwjz-gCJ","colab_type":"code","colab":{}},"source":["# for code implementation\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","from google.colab import drive\n","from torch.optim import SGD, Adam\n","from torch.autograd import Variable\n","# for plotting graphs & data loader\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","from torch.utils.data import DataLoader\n","from torchvision.transforms import Compose, CenterCrop, Normalize\n","from torchvision.transforms import ToTensor, ToPILImage\n","# others\n","from argparse import ArgumentParser\n","# variables - fixed for VOC2012 data set\n","NUM_CHANNELS = 3\n","NUM_CLASSES = 22"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kw4ooOmgA5Xi","colab_type":"text"},"source":["## mount drive, import custom modules\n","dataset.py, criterion.py, transform.py file from [provided link](https://github.com/bodokaiser/piwise)"]},{"cell_type":"code","metadata":{"id":"IurunKduGedm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1592634779301,"user_tz":-540,"elapsed":7393,"user":{"displayName":"Jinha Chung","photoUrl":"","userId":"09027664932398115600"}},"outputId":"30481139-bd4f-4982-f4cc-87c907ba9fe1"},"source":["# mount your Google Drive\n","drive.mount(\"/content/gdrive\", force_remount=True)\n","# copy source files to this directory, to be able to import it directly\n","# if these files are copied and imported without any error,\n","# mounting is successful\n","!cp \"gdrive/My Drive/Project/dataset.py\" .\n","!cp \"gdrive/My Drive/Project/criterion.py\" .\n","!cp \"gdrive/My Drive/Project/transform.py\" .\n","!cp \"gdrive/My Drive/Project/unet.py\" .\n","#!cp \"gdrive/My Drive/Project/attention_unet.py\"\n","from dataset import VOC12\n","from criterion import CrossEntropyLoss2d\n","from transform import Relabel, ToLabel, Colorize\n","#from unet import myUnet\n","import unet\n","#from attention_unet import AttentionUnet"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yuV-rIlQIKXX","colab_type":"text"},"source":["## training\n","the basic structure of this function is also from [provided link](https://github.com/bodokaiser/piwise)"]},{"cell_type":"code","metadata":{"id":"NGuTCDSyI80v","colab_type":"code","colab":{}},"source":["color_transform = Colorize()\n","image_transform = ToPILImage()\n","input_transform = Compose([\n","    CenterCrop(256), # 256x256 cropped image\n","    ToTensor(),\n","    Normalize([.485, .456, .406], [.229, .224, .225]), # mean, std for each channels\n","])\n","target_transform = Compose([\n","    CenterCrop(256), # 256x256 cropped image\n","    ToLabel(),\n","    Relabel(255, 21),\n","])\n","\n","def train(args, model):\n","    # set model to training mode\n","    model.train()\n","    # prepare criterion\n","    weight = torch.ones(22)\n","    weight[0] = 0\n","    # load data - even though Python notebook doesn't really support CLI arguments,\n","    # attributes in args will be assigned manually when calling main() function\n","    loader = DataLoader(VOC12(args.datadir, input_transform, target_transform),\n","        num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n","    # use Adam optimizer\n","    optimizer = Adam(model.parameters())\n","    if args.cuda:\n","        criterion = CrossEntropyLoss2d(weight.cuda())\n","    else:\n","        criterion = CrossEntropyLoss2d(weight)\n","    # start training - epoch values start from 1 to make numbers look 'pretty'\n","    for epoch in range(1, args.num_epochs + 1):\n","        epoch_loss = []\n","        for step, (images, labels) in enumerate(loader):\n","            if args.cuda:\n","                images = images.cuda()\n","                labels = labels.cuda()\n","            inputs = Variable(images)\n","            targets = Variable(labels)\n","            outputs = model(inputs)\n","            # refresh gradient before backprop\n","            optimizer.zero_grad()\n","            loss = criterion(outputs, targets[:, 0])\n","            loss.backward()\n","            optimizer.step()\n","\n","            #epoch_loss.append(loss.data[0])\n","            epoch_loss.append(loss.item())\n","            if (args.steps_loss > 0) and (step % args.steps_loss == 0):\n","                average = sum(epoch_loss) / len(epoch_loss)\n","                print(f'loss: {average} (epoch {epoch}, step {step})')\n","            if (args.steps_save > 0) and (step % args.steps_save == 0):\n","                if args.attention:\n","                    filename = f'AttentionUNet-{epoch:03}-{step:04}.pth'\n","                else:\n","                    filename = f'UNet-{epoch:03}-{step:04}.pth'\n","                torch.save(model.state_dict(), filename)\n","                print(f'save: {filename} (epoch: {epoch}, step: {step})')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLVPfKH2BoLy","colab_type":"text"},"source":["## evaluation"]},{"cell_type":"code","metadata":{"id":"TrPBkTbXBmfL","colab_type":"code","colab":{}},"source":["def evaluate(args, model):\n","    # set model to evaluation mode\n","    model.eval()\n","\n","    image = input_transform(Image.open(args.image))\n","    label = model(Variable(image, volatile=True).unsqueeze(0))\n","    label = color_transform(label[0].data.max(0)[1])\n","\n","    image_transform(label).save(args.label)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OVsOTk5xpDKM","colab_type":"text"},"source":["## main function\n"]},{"cell_type":"code","metadata":{"id":"-hjPXDR7pEff","colab_type":"code","colab":{}},"source":["def main(args):\n","    # load correct model\n","    if args.attention == True:\n","        raise NotImplementedError\n","        #model = AttentionUnet()\n","    elif args.attention == False:\n","        # we don't use residual for non-denoising purposes\n","        model = unet.Unet(in_channels=NUM_CHANNELS,out_channels=NUM_CLASSES\n","                          ,use_residual=False, use_norm=True)\n","    # check if we use GPU\n","    if args.cuda:\n","        model = model.cuda()\n","    # check for mode - training or evaluation?\n","    if args.mode == 'eval':\n","        evaluate(args, model)\n","    elif args.mode == 'train':\n","        train(args, model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WICCzALEwqX1","colab_type":"text"},"source":["## actual running\n","the basic structure of this snippet is also from [provided link](https://github.com/bodokaiser/piwise)"]},{"cell_type":"code","metadata":{"id":"QB9tSNVEwsPd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":433},"executionInfo":{"status":"ok","timestamp":1592636071115,"user_tz":-540,"elapsed":443032,"user":{"displayName":"Jinha Chung","photoUrl":"","userId":"09027664932398115600"}},"outputId":"983d5918-eb94-42c5-a3ed-83fc71bcee45"},"source":["if __name__ == '__main__':\n","    # comment out lines from original code that is not used here\n","    parser = ArgumentParser()\n","    parser.add_argument('--cuda', action='store_true')\n","    parser.add_argument('--attention', action='store_true')\n","    #parser.add_argument('--state')\n","\n","    subparsers = parser.add_subparsers(dest='mode')\n","    subparsers.required = True\n","\n","    parser_eval = subparsers.add_parser('eval')\n","    parser_eval.add_argument('image')\n","    parser_eval.add_argument('label')\n","\n","    parser_train = subparsers.add_parser('train')\n","    #parser_train.add_argument('--port', type=int, default=80)\n","    parser_train.add_argument('--datadir', required=True)\n","    parser_train.add_argument('--num-epochs', type=int, default=32)\n","    parser_train.add_argument('--num-workers', type=int, default=4)\n","    parser_train.add_argument('--batch-size', type=int, default=1)\n","    parser_train.add_argument('--steps-loss', type=int, default=50)\n","    #parser_train.add_argument('--steps-plot', type=int, default=0)\n","    parser_train.add_argument('--steps-save', type=int, default=500)\n","    # since we can't pass CLI arugments in Python notebook,\n","    # assign variables manually\n","    # add two spaces between each chunk so that the data directory doesn't get split\n","    command_line = \"--cuda  train  --datadir  gdrive/My Drive/Project/data  --num-epochs  1  --num-workers  4  --batch-size  4  --steps-save  500\"\n","    args = parser.parse_args(command_line.split(\"  \"))\n","    print(args)\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Namespace(attention=False, batch_size=4, cuda=True, datadir='gdrive/My Drive/Project/data', mode='train', num_epochs=1, num_workers=4, steps_loss=50, steps_save=500)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:211: UserWarning: NLLLoss2d has been deprecated. Please use NLLLoss instead as a drop-in replacement and see https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.\n","  warnings.warn(\"NLLLoss2d has been deprecated. \"\n","/content/criterion.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return self.loss(F.log_softmax(outputs), targets)\n"],"name":"stderr"},{"output_type":"stream","text":["loss: 3.0050792694091797 (epoch 1, step 0)\n","save: UNet-001-0000.pth (epoch: 1, step: 0)\n","loss: 2.958860387989119 (epoch 1, step 50)\n","loss: 2.903348134295775 (epoch 1, step 100)\n","loss: 2.891738574236434 (epoch 1, step 150)\n","loss: 2.85823853098931 (epoch 1, step 200)\n","loss: 2.8436008544557123 (epoch 1, step 250)\n","loss: 2.840546433711765 (epoch 1, step 300)\n","loss: 2.827510713512062 (epoch 1, step 350)\n","loss: 2.8133812738475656 (epoch 1, step 400)\n","loss: 2.798139847831557 (epoch 1, step 450)\n","loss: 2.790436963359277 (epoch 1, step 500)\n","save: UNet-001-0500.pth (epoch: 1, step: 500)\n","loss: 2.7839086546871927 (epoch 1, step 550)\n","loss: 2.7703526426670755 (epoch 1, step 600)\n","loss: 2.7653924054630705 (epoch 1, step 650)\n","loss: 2.7572788656523155 (epoch 1, step 700)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8OsCd1d97ky2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1592576077685,"user_tz":-540,"elapsed":3013,"user":{"displayName":"Jinha Chung","photoUrl":"","userId":"09027664932398115600"}},"outputId":"8ba141df-fda6-4e73-f618-bfa7b1fcb668"},"source":["!ls /content/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["criterion.py  gdrive\t   sample_data\t unet.py\n","dataset.py    __pycache__  transform.py\n"],"name":"stdout"}]}]}